bmsce@bmsce-OptiPlex-3060:~$ spark-shell
22/06/28 09:35:41 WARN Utils: Your hostname, bmsce-OptiPlex-3060 resolves to a loopback address: 127.0.1.1; using 10.124.7.99 instead (on interface enp1s0)
22/06/28 09:35:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/06/28 09:35:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://10.124.7.99:4040
Spark context available as 'sc' (master = local[*], app id = local-1656389146702).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val data=sc.textFile("sparkdata.txt")
data: org.apache.spark.rdd.RDD[String] = sparkdata.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> data.collect;  
res0: Array[String] = Array(Scala combines object-oriented and functional programming in one concise, high-level language. Scala's static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries.)

scala> val splitdata = data.flatMap(line => line.split(" "));  
splitdata: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> splitdata.collect;  
res1: Array[String] = Array(Scala, combines, object-oriented, and, functional, programming, in, one, concise,, high-level, language., Scala's, static, types, help, avoid, bugs, in, complex, applications,, and, its, JVM, and, JavaScript, runtimes, let, you, build, high-performance, systems, with, easy, access, to, huge, ecosystems, of, libraries.)

scala> val mapdata = splitdata.map(word => (word,1));  
mapdata: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:25

scala> mapdata.collect;  
res2: Array[(String, Int)] = Array((Scala,1), (combines,1), (object-oriented,1), (and,1), (functional,1), (programming,1), (in,1), (one,1), (concise,,1), (high-level,1), (language.,1), (Scala's,1), (static,1), (types,1), (help,1), (avoid,1), (bugs,1), (in,1), (complex,1), (applications,,1), (and,1), (its,1), (JVM,1), (and,1), (JavaScript,1), (runtimes,1), (let,1), (you,1), (build,1), (high-performance,1), (systems,1), (with,1), (easy,1), (access,1), (to,1), (huge,1), (ecosystems,1), (of,1), (libraries.,1))

scala> val reducedata = mapdata.reduceByKey(_+_);  
reducedata: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25

scala> reducedata.collect;
res3: Array[(String, Int)] = Array((concise,,1), (its,1), (combines,1), (object-oriented,1), (build,1), (one,1), (with,1), (Scala's,1), (complex,1), (static,1), (access,1), (easy,1), (Scala,1), (systems,1), (language.,1), (runtimes,1), (applications,,1), (you,1), (avoid,1), (types,1), (JavaScript,1), (let,1), (high-level,1), (bugs,1), (help,1), (high-performance,1), (to,1), (huge,1), (in,2), (of,1), (programming,1), (and,3), (functional,1), (libraries.,1), (ecosystems,1), (JVM,1))
